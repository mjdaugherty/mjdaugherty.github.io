<!DOCTYPE html>

<html>
	<head>
		<title>Exploratory Data Analysis from Scratch</title>
		<meta name="description" content="This is a blog post.">
		<meta name="author" content="Mike">
	</head>
	<body bgcolor="#000000">
		<font face="monospace" size="5" color="#BABDB6">Exploratory
		Data Analysis from Scratch</font>
		<br>
		<br>
		<font face="monospace" size="4" color="#BABDB6">
			Exploratory data analysis, often referred to simply as EDA, is one of the most important parts of any data science project, and in my opinion it is also one of the most fun. EDA is where our creativity comes into play: we familiarize ourselves with the data and come up with interesting questions in the process. We often discover things we did not expect. We have to write a lot of code and do a lot of Googling. In this post, I will attempt to outline the first few steps of the data science workflow, starting with a blank text editor and ending with a few questions and possible answers.
		<br>
		<br>
		The first step, as always, is our imports. For this project, we're going to need requests, time, numpy, pandas and matplotlib's pyplot module:
		<br>
		<br>
		<img src=
			"https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/imports.png"
			alt="importing requests, time, numpy, pandas and
			matplotlib.pyplot">
		<br>
		<br>
		The next step is to prepare our data. Sometimes we will be provided with a nice clean csv or Excel spreadsheet to work with, but more often we'll have to collect and convert the data into a usable form all on our own. I'm going to do this with some good old-fashioned web scraping (with the help of <a href="https://ned.ipac.caltech.edu/Documents/Guides/Interface/ObjectLookup">The NASA/IPAC Extragalactic Database's excellent API</a>):
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/url.png" alt="requests.get('http://ned.ipac.caltech.edu/srs/ObjectLookup?name=ngc891'">
		<br>
		<br>
		The above image shows how to use Python's requests library to make a request to an HTTP server, which is the first step in gathering data from the web. I first store the base URL that I will use for all of my queries in a variable called url. I then make a request to the server using the base URL and the query string "ngc891" and store the server's response in another variable, res. Finally, I check the status code of the response to make sure that the request was successful; <a href="https://en.wikipedia.org/wiki/List_of_HTTP_status_codes">200</a> means we're good to go. This all could have been done in a single line of code and without creating any variables, but the reason for using them will become clear in a minute.
		<br>
		<br>
		Now, I'm going to take that res variable and turn it into something I can use. Python has a built-in function that can turn a response object into a JSON object, which for our purposes we can safely consider the same as a dictionary. (In fact, if you check the type of a JSON-ified response object in Python, it is dict.) And since it is a dict, we can check out the keys:
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/resultcode.png" alt="keys: ['Copyright', 'QueryTime', 'Version', 'Preferred', 'Supplied', 'Interpreted', 'NameResolver', 'StatusCode', 'ResultCode']">
		<br>
		<br>
		I won't go into the details here, but after examining all of these keys (many of whose associated values were themselves dictionaries whose keys were attached to dictionaries which had keys pointing to dictionaries and...), I found which ones gave me the values I was after and put them in a loop.
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/scraping.png" alt="for loop to automate requests">
		<br>
		<br>
		#1: Create an empty list to hold my data entries.
		<br>
		#2: Create a variable to hold my query string and update at each iteration through the loop (7840 in total).
		<br>
		#3: Make my HTTP request.
		<br>
		#4: Turn the response into a dictionary.
		<br>
		#5: Check the value of the ResultCode (one of the keys of the dictionary, similar to the status code but unique to this particular API and a better value to check in this case).
		<br>
		#6: Check that the thing I'm pulling is a galaxy; if so:
		<br>
		#7: Make a list with the values I want to grab for the object.
		<br>
		#8: Append the list of values to my list of entries.
		<br>
		#9: Move on to the next object if the ResultCode is wrong.
		<br>
		#10: Pause for 1.1 seconds. This database allows automated queries - <b>always check a website's robots.txt</b> to make sure of this before scraping!) - but requests that users not exceed one per second.
		#11 (not included in screencap): Fix yourself a drink and get comfortable because this thing is going to be running for quite a while.
		<br>
		<br>
		Now, finally, I have my data in the form of a giant list of lists, which means I can turn it into a pandas DataFrame:
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/dataframe.png" alt="galaxies = pd.DataFrame(data=galaxies, columns=['name', 'ra', 'dec', 'z', '+/-']">
		<br>
		<br>
		I made a DataFrame called galaxies from a list called galaxies. Whoops. Anyway, here's what the column names mean:
		<br>
		name >> <a href="https://en.wikipedia.org/wiki/New_General_Catalogue">NGC number</a> (or <a href="https://en.wikipedia.org/wiki/List_of_Messier_objects">Messier number</a>, if it has one)
		<br>
		ra >> <a href="https://en.wikipedia.org/wiki/Right_ascension">right ascension</a>
		<br>
		dec >> <a href="https://en.wikipedia.org/wiki/Declination">declination</a>
		<br>
		z >> <a href="https://en.wikipedia.org/wiki/Redshift">redshift</a>
		<br>
		+/- >> uncertainty in redshift measurement
		<br>
		<br>
		There are two pandas methods that I always like to run first when looking at new data, .head() and .describe(), which show the first five rows of a DataFrame and give a tidy summary of the data, respectively:
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/galaxies.png" alt="galaxies.head()">
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/galaxies2.png" alt="galaxies.describe()">
		<br>
		<br>
		The values for right ascension range from 0 to 360, and those for declination range from -90 to 90; given how these numbers are defined, the values make sense. But we can see that there are some missing values, so I'm going to see how many there are:
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/nans.png" alt="57 missing z values and 104 missing +/- values">
		<br>
		<br>
		What to do with null values is up to you. Since I'm just playing around here, I'm going to leave them alone, but if you're working on a real problem you will have to consider your options and deal with missing data appropriately.
		<br>
		<br>
		Another thing worth noting is that there are negative redshift values:
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/negatives.png">
		<br>
		<br>
		Redshift is the amount of change we observe in the wavelength of light emitted by objects that are moving relative to our frame of reference. The higher the redshift, the faster the relative motion. Negative values indicate that the object is moving toward us. I'm not familiar with every galaxy in the above image, but it includes eight Messier galaxies, which are relatively close to the Milky Way and so it isn't unreasonable to think that they are being pulled toward our galaxy by gravity. This is an example of how knowing a bit about the thing your data describe can help you decide what is a red flag and what isn't.
		<br>
		<br>
		Now I'd like to check out the distributions of these galaxies' angular coordinates with some histograms.
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/dechist.png" alt="declination histogram">
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/rahist.png" alt="right ascension histogram">
		<br>
		<br>
		The right ascension plot is very interesting. While the declination measurements appear to follow a nearly normal distribution centered just north of the celestial equator (a result of the location on Earth where the observations were made), the right ascension measurements show one high peak in the middle and two smaller ones at the ends. This is actually a bit deceptive because, right ascension being an angular measure, 0째 and 360째 are the same value, so the two peaks at the ends actually "wrap around" to meet each other. And so we have a ton of galaxies clustered up around 180째 (12 hours) of right ascension as well as a bunch of galaxies spread over a patch of sky a little less than 100째 wide - from, say, the eastern horizon up to the zenith. My guess is that the first group represents the <a href="https://en.wikipedia.org/wiki/Virgo_Cluster">Virgo Cluster</a>; a quick glance at some star charts seems to back up that supposition. I'm not sure about the other group, but it would be simple enough to find an explanation. This is another instance where having a little knowledge about the data you're looking at can go a long way.
		<br>
		<br>
		I'll make a scatterplot of the coordinates too.
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/scatter.png" alt="scatterplot of right ascension and decliation">
		<br>
		<br>
		I'll be honest: I don't know why this plot looks the way it does, but I really like it. I was not expecting to see those black voids in there, so that's definitely something to investigate.
		There is so much more that one could do with this dataset, but I'm going to stop here before this post gets even longer than it already is. I hope you found something useful or interesting in it. Thanks for reading.
		</font>
	</body>
</html>
