<!DOCTYPE html>

<html>
	<head>
		<title>Exploratory Data Analysis from Scratch</title>
		<meta name="description" content="This is a blog post.">
		<meta name="author" content="Mike">
	</head>
	<body bgcolor="#000000">
		<font face="monospace" size="5" color="#BABDB6">Exploratory
		Data Analysis from Scratch</font>
		<br>
		<br>
		<font face="monospace" size="4" color="#BABDB6">
			Exploratory data analysis, often referred to simply as EDA, is one of the most important parts of any data science project, and in my opinion it is also one of the most fun. EDA is where our creativity comes into play: we familiarize ourselves with the data and come up with interesting questions in the process. We often discover things we did not expect. We have to write a lot of code and do a lot of Googling. In this post, I will attempt to outline the first few steps of the data science workflow, starting with a blank text editor and ending with a few questions and possible answers.
		<br>
		<br>
		The first step, as always, is our imports. For this project, we're going to need requests, time, numpy, pandas and matplotlib's pyplot module:
		<br>
		<br>
		<img src=
			"https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/imports.png"
			alt="importing requests, time, numpy, pandas and
			matplotlib.pyplot">
		<br>
		<br>
		The next step is to prepare our data. Sometimes we will be provided with a nice clean csv or Excel spreadsheet to work with, but more often we'll have to collect and convert the data into a usable form all on our own. I'm going to do this with some good old-fashioned web scraping (with the help of <a href="https://ned.ipac.caltech.edu/Documents/Guides/Interface/ObjectLookup">The NASA/IPAC Extragalactic Database's excellent API</a>):
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/url.png" alt="requests.get('http://ned.ipac.caltech.edu/srs/ObjectLookup?name=ngc891'">
		<br>
		<br>
		The above image shows how to use Python's requests library to make a request to an HTTP server, which is the first step in gathering data from the web. I first store the base URL that I will use for all of my queries in a variable called url. I then make a request to the server using the base URL and the query string "ngc891" and store the server's response in another variable, res. Finally, I check the status code of the response to make sure that the request was successful; <a href="https://en.wikipedia.org/wiki/List_of_HTTP_status_codes">200</a> means we're good to go. This all could have been done in a single line of code and without creating any variables, but the reason for using them will become clear in a minute.
		<br>
		<br>
		Now, I'm going to take that res variable and turn it into something I can use. Python has a built-in function that can turn a response object into a JSON object, which for our purposes we can safely consider the same as a dictionary. (In fact, if you check the type of a JSON-ified response object in Python, it is dict.) And since it is a dict, we can check out the keys:
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/resultcode.png" alt="keys: ['Copyright', 'QueryTime', 'Version', 'Preferred', 'Supplied', 'Interpreted', 'NameResolver', 'StatusCode', 'ResultCode']">
		<br>
		<br>
		I won't go into the details here, but after examining all of these keys (many of whose associated values were themselves dictionaries whose keys were attached to dictionaries which had keys pointing to dictionaries and...), I found which ones gave me the values I was after and put them in a loop.
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/scraping.png" alt="for loop to automate requests">
		<br>
		<br>
		#1: Create an empty list to hold my data entries.
		<br>
		#2: Create a variable to hold my query string and update at each iteration through the loop (7840 in total).
		<br>
		#3: Make my HTTP request.
		<br>
		#4: Turn the response into a dictionary.
		<br>
		#5: Check the value of the ResultCode (one of the keys of the dictionary, similar to the status code but unique to this particular API and a better value to check in this case).
		<br>
		#6: Check that the thing I'm pulling is a galaxy; if so:
		<br>
		#7: Make a list with the values I want to grab for the object.
		<br>
		#8: Append the list of values to my list of entries.
		<br>
		#9: Move on to the next object if the ResultCode is wrong.
		<br>
		#10: Pause for 1.1 seconds. This database allows automated queries - <b>always check a website's robots.txt</b> to make sure of this before scraping!) - but requests that users not exceed one per second.
		#11 (not included in screencap): Fix yourself a drink and get comfortable because this thing is going to be running for quite a while.
		<br>
		<br>
		Now, finally, I have my data in the form of a giant list of lists, which means I can turn it into a pandas DataFrame:
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/dataframe.png" alt="galaxies = pd.DataFrame(data=galaxies, columns=['name', 'ra', 'dec', 'z', '+/-']">
		<br>
		<br>
		I made a DataFrame called galaxies from a list called galaxies. Whoops. Anyway, here's what the column names mean:
		<br>
		name >> <a href="https://en.wikipedia.org/wiki/New_General_Catalogue">NGC number</a> (or <a href="https://en.wikipedia.org/wiki/List_of_Messier_objects">Messier number</a>, if it has one)
		<br>
		ra >> <a href="https://en.wikipedia.org/wiki/Right_ascension">right ascension</a>
		<br>
		dec >> <a href="https://en.wikipedia.org/wiki/Declination">declination</a>
		<br>
		z >> <a href="https://en.wikipedia.org/wiki/Redshift">redshift</a>
		<br>
		+/- >> uncertainty in redshift measurement
		<br>
		<br>
		There are two pandas methods that I always like to run first when looking at new data, .head() and .describe(), which show the first five rows of a DataFrame and give a tidy summary of the data, respectively:
		<br>
		<br>
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/galaxies.png" alt="galaxies.head()">
		<img src="https://raw.githubusercontent.com/mjdaugherty/mjdaugherty.github.io/master/images/galaxies2.png" alt="galaxies.describe()">
		<br>
		<br>
		The values for right ascension range from 0 to 360, and those for declination range from -90 to 90; given how these numbers are defined, the values make sense. But there is at least one negative redshift value, which doesn't make sense, and we can see that some are missing entirely, so it would be wise to investigate those:
		</font>
	</body>
</html>
